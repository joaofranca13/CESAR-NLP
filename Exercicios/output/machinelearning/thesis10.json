{"title": "Time series forecasting with deep forest regression", "abstract": "A time series is a collection of ordered observations which are usually measured in\r\nrepeated intervals. Time series forecasting is an area of research which studies\r\nmethods for prediction of future values in a series. Forecasting methods range from\r\nstatistical procedures, such as ARIMA to, more recently, machine learning\r\napproaches. Deep neural networks (DNNs) have shown good performance on a\r\ngreat number of tasks, including time series forecasting for which DNNs are\r\nconsidered state-of-the-art. Most deep models today are neural networks, but despite\r\nits popularity and proven competitive performance when compared to other machine\r\nlearning algorithms, DNNs still face some limitations. Most notably, they usually\r\nrequire a large number of training examples - which could be unavailable for smaller\r\ntime series - and they possess a large number of hyper-parameters which need to be\r\ntuned to individual datasets. Multi-grained cascade forest (gcForest) is a deep\r\nmachine learning algorithm which has been proposed for classification and that\r\naddresses DNNs limitations while replicating the features which are responsible for\r\nthe success of this type of model. This dissertation\u2019s goal is to adapt the original\r\ngcForest algorithm in order for it to work with regression problems, enabling it to be\r\napplied to time series forecasting. The influence of the two different stages of\r\ngcForest - multi-grained scanning and cascade forest, is also investigated. Also\r\nexplored is the possibility of adding an additional model to the end of the cascade\r\nforest structure and thus change the way the final result is calculated. Changes to the\r\nalgorithm are presented and its performance is evaluated on four different time series\r\ndatasets, according to three performance metrics: mean squared error, mean\r\nabsolute error and mean absolute percentage error. Results show that gcForest\r\nachieves competitive performance on all four datasets, when compared to traditional\r\nmachine learning models."}